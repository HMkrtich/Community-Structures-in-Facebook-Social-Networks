{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Unsupervised_models.models import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from sklearn.cluster import KMeans, DBSCAN, MeanShift, AgglomerativeClustering, OPTICS,  estimate_bandwidth\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.cm as cm\n",
    "import networkx as nx\n",
    "import networkx.algorithms.community as nxcom\n",
    "import community as community_louvain\n",
    "from networkx.algorithms.community import k_clique_communities, modularity\n",
    "# Constants\n",
    "EGO_NODES = [ 1684, 1912, 3437, 3980]\n",
    "# 0, 348, 414, 686, 698,\n",
    "ONLY_FEATURE_PARAMS = {0 : {'KMeans':{\"k\": 100}, 'DBSCAN':{\"eps\": 0.05, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 10, \"covariance_type\": 'full'}, 'Hier':{\"k\": 50, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 5, \"xi\": 0.05, \"min_cluster_size\": 0.05}},\n",
    "                       107 : {'KMeans':{\"k\": 100}, 'DBSCAN':{\"eps\": 0.07, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 3}, 'Hier':{\"k\": 50, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 6, \"xi\": 0.05, \"min_cluster_size\": 0.05}},\n",
    "                          348 : {'KMeans':{\"k\": 100}, 'DBSCAN':{\"eps\": 0.05, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 3}, 'Hier':{\"k\": 60, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 5, \"xi\": 0.05, \"min_cluster_size\": 0.05}},\n",
    "                            414 : {'KMeans':{\"k\": 100}, 'DBSCAN':{\"eps\": 0.05, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 3}, 'Hier':{\"k\": 70, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 5, \"xi\": 0.05, \"min_cluster_size\": 0.05}},\n",
    "                                686 : {'KMeans':{\"k\": 100}, 'DBSCAN':{\"eps\": 0.05, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 3}, 'Hier':{\"k\": 65, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 5, \"xi\": 0.05, \"min_cluster_size\": 0.05}},\n",
    "                                    698 : {'KMeans':{\"k\": 60}, 'DBSCAN':{\"eps\": 0.05, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 3}, 'Hier':{\"k\": 65, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 5, \"xi\": 0.05, \"min_cluster_size\": 0.05}},\n",
    "                                    1684 : {'KMeans':{\"k\": 100}, 'DBSCAN':{\"eps\": 0.05, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 3}, 'Hier':{\"k\": 80, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 5, \"xi\": 0.05, \"min_cluster_size\": 0.05}},\n",
    "                                        1912 : {'KMeans':{\"k\": 100}, 'DBSCAN':{\"eps\": 0.05, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 3}, 'Hier':{\"k\": 80, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 5, \"xi\": 0.05, \"min_cluster_size\": 0.05}},\n",
    "                                        3437 : {'KMeans':{\"k\": 100}, 'DBSCAN':{\"eps\": 0.05, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 3}, 'Hier':{\"k\": 80, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 5, \"xi\": 0.05, \"min_cluster_size\": 0.05}},\n",
    "                                            3980 : {'KMeans':{\"k\": 58}, 'DBSCAN':{\"eps\": 0.05, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 3}, 'Hier':{\"k\": 58, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 5, \"xi\": 0.05, \"min_cluster_size\": 0.05}}}\n",
    "\n",
    "# Model types\n",
    "class Models(Enum):\n",
    "    KMEANS = 1\n",
    "    DBSCAN = 2\n",
    "    MEANSHIFT = 3\n",
    "    GMM = 4\n",
    "    HIERS = 5\n",
    "    Greedy = 6\n",
    "\n",
    "\n",
    "class ModelTypes(Enum):\n",
    "    ONLY_FEEATURES = 1\n",
    "    ONLY_EDGES = 2\n",
    "    EDGES_AND_FEATURES = 3\n",
    "\n",
    "# Unsupervised models\n",
    "class UnsupservisedModels:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_model(self, model):\n",
    "        \n",
    "        if model == Models.KMEANS:\n",
    "            return My_KMeans()\n",
    "        elif model == Models.DBSCAN:\n",
    "            return MY_DBSCAN()\n",
    "        # elif model == Models.MEANSHIFT:\n",
    "        #     return MeanShift()\n",
    "        # elif model == Models.GMM:\n",
    "        #     return GaussianMixture()\n",
    "        # elif model == Models.HIERS:\n",
    "        #     return AgglomerativeClustering()\n",
    "        else:\n",
    "            raise ValueError(\"Model not found\")\n",
    "\n",
    "# KMeans\n",
    "class My_KMeans:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        print(\"KMeans initialized\")\n",
    "        \n",
    "    def train(self, data):\n",
    "        print(\"Training KMeans\")\n",
    "        self.model = KMeans(n_clusters=self.k)\n",
    "\n",
    "        self.data = data\n",
    "        self.model.fit(data)\n",
    "\n",
    "        self.centroids = self.model.cluster_centers_\n",
    "        self.labels = self.model.labels_\n",
    "\n",
    "    # Get results\n",
    "    def get_results(self):\n",
    "        return self.labels\n",
    "    \n",
    "# DBSCAN\n",
    "class My_DBSCAN:\n",
    "    def __init__(self, eps, min_samples):\n",
    "        self.eps = 0.5\n",
    "        self.min_samples = 3\n",
    "        # Initialize DBSCAN\n",
    "        self.model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "\n",
    "    def train(self, data):\n",
    "        self.data = data\n",
    "        self.model.fit(data)\n",
    "\n",
    "        self.labels = self.model.labels_\n",
    "\n",
    "    # Get results\n",
    "    def get_results(self):\n",
    "        return self.labels\n",
    "    \n",
    "# MeanShift\n",
    "class My_MeanShift:\n",
    "    def __init__(self) -> None:\n",
    "        self.model = MeanShift()\n",
    "\n",
    "    def train(self, data):\n",
    "        self.data = data\n",
    "        self.model.fit(data)\n",
    "\n",
    "        self.labels = self.model.labels_\n",
    "\n",
    "    # Get results\n",
    "    def get_results(self):\n",
    "        return self.labels\n",
    "    \n",
    "# GaussianMixture\n",
    "class My_GMM:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.model = GaussianMixture(self.k)\n",
    "\n",
    "    def train(self, data):\n",
    "        self.data = data\n",
    "        self.model.fit(data)\n",
    "\n",
    "        self.labels = self.model.predict(data)\n",
    "\n",
    "    # Get results\n",
    "    def get_results(self):\n",
    "        return self.labels\n",
    "    \n",
    "# AgglomerativeClustering\n",
    "class My_Hier:\n",
    "    def __init__(self, k, affinity, linkage):\n",
    "        self.k = k\n",
    "        self.affinity = affinity\n",
    "        self.linkage = linkage\n",
    "\n",
    "        self.model = AgglomerativeClustering(n_clusters=self.k, affinity=self.affinity, linkage=self.linkage)\n",
    "\n",
    "    def train(self, data):\n",
    "        self.data = data\n",
    "        self.model.fit(data)\n",
    "\n",
    "        self.labels = self.model.labels_\n",
    "\n",
    "    # Get results\n",
    "    def get_results(self):\n",
    "        return self.labels\n",
    "    \n",
    "# OPTICS\n",
    "class My_OPTICS:\n",
    "    def __init__(self, min_samples=5, xi=0.05, min_cluster_size=0.05):\n",
    "        self.model = OPTICS(min_samples=min_samples, xi=xi, min_cluster_size=min_cluster_size)\n",
    "\n",
    "    def train(self, data):\n",
    "        self.data = data\n",
    "        self.model.fit(data)\n",
    "        self.labels = self.model.labels_\n",
    "\n",
    "    def get_results(self):\n",
    "        return self.labels\n",
    "\n",
    "\n",
    "# Helpers\n",
    "def get_results(G, data, data_dict, index_dict, labels, model_type, ego, plot=False):\n",
    "    if model_type == ModelTypes.ONLY_FEEATURES:\n",
    "        kmeans_labels, dbscan_labels, meanshift_labels, gmm_labels, hier_labels, optics_labels = only_features_models(data, index_dict, ego)\n",
    "        if plot:\n",
    "            print('KMeans')\n",
    "            visualize(G, kmeans_labels, \"KMeans\")\n",
    "            print('DBSCAN')\n",
    "            visualize(G, dbscan_labels, \"DBSCAN\")\n",
    "            print('MeanShift')\n",
    "            visualize(G, meanshift_labels, \"MeanShift\")\n",
    "            print('GMM')\n",
    "            visualize(G, gmm_labels, \"GMM\")\n",
    "            print('Hier')\n",
    "            visualize(G, hier_labels, \"Hier\")\n",
    "            print('OPTICS')\n",
    "            visualize(G, optics_labels, \"OPTICS\")\n",
    "    \n",
    "        \n",
    "        ars_k_means, nmis_k_means, hs_k_means, cs_k_means, vs_k_means, ber_kmeans = test_labels_results(kmeans_labels, labels)\n",
    "        ars_dbscan, nmis_dbscan, hs_dbscan, cs_dbscan, vs_dbscan, ber_dbscan = test_labels_results(dbscan_labels, labels)\n",
    "        ars_meanshift, nmis_meanshift, hs_meanshift, cs_meanshift, vs_meanshift, ber_meanshift = test_labels_results(meanshift_labels, labels)\n",
    "        ars_gmm, nmis_gmm, hs_gmm, cs_gmm, vs_gmm, ber_gmm = test_labels_results(gmm_labels, labels)\n",
    "        ars_hier, nmis_hier, hs_hier, cs_hier, vs_hier, ber_hier = test_labels_results(hier_labels, labels)\n",
    "        ars_optics, nmis_optics, hs_optics, cs_optics, vs_optics, ber_optics = test_labels_results(optics_labels, labels)\n",
    "        \n",
    "        # Create dataframe for the results with index as the model name and columns as the metrics\n",
    "        results =  pd.DataFrame(index=['KMeans', 'DBSCAN', 'MeanShift', 'GMM', 'Hier', 'OPTICS'],\n",
    "                                 columns=['Adjusted Rand Score', 'Normalized Mutual Info Score', 'Homogeneity Score',\n",
    "                                            'Completeness Score', 'V Measure Score', 'BER'])\n",
    "        results.loc['KMeans'] = [ars_k_means, nmis_k_means, hs_k_means, cs_k_means, vs_k_means, ber_kmeans]\n",
    "        results.loc['DBSCAN'] = [ars_dbscan, nmis_dbscan, hs_dbscan, cs_dbscan, vs_dbscan, ber_dbscan]\n",
    "        results.loc['MeanShift'] = [ars_meanshift, nmis_meanshift, hs_meanshift, cs_meanshift, vs_meanshift, ber_meanshift]\n",
    "        results.loc['GMM'] = [ars_gmm, nmis_gmm, hs_gmm, cs_gmm, vs_gmm, ber_gmm]\n",
    "        results.loc['Hier'] = [ars_hier, nmis_hier, hs_hier, cs_hier, vs_hier, ber_hier]\n",
    "        results.loc['OPTICS'] = [ars_optics, nmis_optics, hs_optics, cs_optics, vs_optics, ber_optics]\n",
    "    \n",
    "\n",
    "        return results\n",
    "        \n",
    "        # return pass\n",
    "    \n",
    "    elif model_type == ModelTypes.ONLY_EDGES:\n",
    "\n",
    "        results_greedy, results_newman, results_partition, results_label_propagation, results_asyn_lpa_communities = only_edges_models(G)\n",
    "\n",
    "\n",
    "        resss= get_data_from_communities(results_greedy)\n",
    "        ars_greedy, nmis_greedy, hs_greedy, cs_greedy, vs_greedy, ber_greedy = test_labels_results(resss, labels)\n",
    "        \n",
    "        print('Newman')\n",
    "        for communities in results_newman:\n",
    "            partition = list(map(set, communities))\n",
    "            print(\"Size: \", len(partition))\n",
    "       \n",
    "        \n",
    "        ars_newman, nmis_newman, hs_newman, cs_newman, vs_newman, ber_newman = test_labels_results(get_data_from_communities(best_partition), labels)\n",
    "\n",
    "        ars_partition, nmis_partition, hs_partition, cs_partition, vs_partition, ber_partition = test_labels_results(results_partition, labels)\n",
    "\n",
    "        ars_propogation, nmis_propogation, hs_propogation, cs_propogation, vs_propogation, ber_propogation = test_labels_results(get_data_from_communities(results_label_propagation), labels)\n",
    "\n",
    "        ars_lpa, nmis_lpa, hs_lpa, cs_lpa, vs_lpa, ber_lpa = test_labels_results(get_data_from_communities(results_asyn_lpa_communities), labels)\n",
    "       \n",
    "        if plot:\n",
    "            print('Greedy')\n",
    "            visualize(G, resss, \"Greedy\")\n",
    "            print('Newman')\n",
    "            visualize(G, get_data_from_communities(best_partition), \"Newman\")\n",
    "            print('Partition')\n",
    "            visualize(G, results_partition, \"Partition\")\n",
    "            print('Label Propogation')\n",
    "            visualize(G, get_data_from_communities(results_label_propagation), \"Label Propogation\")\n",
    "            print('LPA')\n",
    "            visualize(G, get_data_from_communities(results_asyn_lpa_communities), \"LPA\")\n",
    "        \n",
    "        # Create dataframe for the results with index as the model name and columns as the metrics\n",
    "        results =  pd.DataFrame(index=['Greedy', 'Newman', 'Partition', 'Label Propogation', 'LPA'],\n",
    "                                 columns=['Adjusted Rand Score', 'Normalized Mutual Info Score', 'Homogeneity Score',\n",
    "                                            'Completeness Score', 'V Measure Score', 'BER'])\n",
    "        \n",
    "        results.loc['Greedy'] = [ars_greedy, nmis_greedy, hs_greedy, cs_greedy, vs_greedy, ber_greedy]\n",
    "        results.loc['Newman'] = [ars_newman, nmis_newman, hs_newman, cs_newman, vs_newman, ber_newman]\n",
    "        results.loc['Partition'] = [ars_partition, nmis_partition, hs_partition, cs_partition, vs_partition, ber_partition]\n",
    "        results.loc['Label Propogation'] = [ars_propogation, nmis_propogation, hs_propogation, cs_propogation, vs_propogation, ber_propogation]\n",
    "        results.loc['LPA'] = [ars_lpa, nmis_lpa, hs_lpa, cs_lpa, vs_lpa, ber_lpa]\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "        \n",
    "        # return resss\n",
    "        # return only_edges_models(data, labels)\n",
    "    elif model_type == ModelTypes.EDGES_AND_FEATURES:\n",
    "        return edges_and_features_models(data, labels)\n",
    "    else:\n",
    "        raise ValueError(\"Model type not found\")\n",
    "    \n",
    "# Only features models\n",
    "def only_features_models(data, index_dict, ego):\n",
    "    \"\"\"\n",
    "    data - data\n",
    "    labels - labels\n",
    "    \n",
    "    We are going to use KMeans, DBSCAN, MeanShift, GaussianMixture, AgglomerativeClustering\n",
    "    \"\"\"\n",
    "\n",
    "    # KMeans\n",
    "    kmeans = KMeans(n_clusters=ONLY_FEATURE_PARAMS[ego]['KMeans']['k'])\n",
    "    kmeans.fit(data)\n",
    "    kmeans_labels = get_labels_dir(kmeans.labels_, index_dict)\n",
    "\n",
    "    # DBSCAN\n",
    "    dbscan = DBSCAN(eps=ONLY_FEATURE_PARAMS[ego]['DBSCAN']['eps'], min_samples=ONLY_FEATURE_PARAMS[ego]['DBSCAN']['min_samples'])\n",
    "    dbscan.fit(data)\n",
    "    dbscan_labels = get_labels_dir(dbscan.labels_, index_dict)\n",
    "\n",
    "    # MeanShift\n",
    "    bandwidth = estimate_bandwidth(data)\n",
    "\n",
    "    meanshift = MeanShift(bandwidth=bandwidth)\n",
    "    meanshift.fit(data)\n",
    "    meanshift_labels = get_labels_dir(meanshift.labels_, index_dict)\n",
    "\n",
    "    # GaussianMixture\n",
    "    gmm = GaussianMixture(n_components=3, covariance_type='tied', tol=0.001, reg_covar=1e-06, init_params='random', max_iter=200, n_init=3)\n",
    "    gmm.fit(data)\n",
    "    gmm_labels = get_labels_dir(gmm.predict(data), index_dict)\n",
    "\n",
    "    # AgglomerativeClustering\n",
    "    hier = AgglomerativeClustering(n_clusters=ONLY_FEATURE_PARAMS[ego]['Hier']['k'], affinity=ONLY_FEATURE_PARAMS[ego]['Hier']['affinity'], linkage=ONLY_FEATURE_PARAMS[ego]['Hier']['linkage'])\n",
    "    hier.fit(data)\n",
    "    hier_labels = get_labels_dir(hier.labels_, index_dict)\n",
    "\n",
    "    # OPTICS\n",
    "    optics = OPTICS(min_samples=ONLY_FEATURE_PARAMS[ego]['OPTICS']['min_samples'], xi=ONLY_FEATURE_PARAMS[ego]['OPTICS']['xi'], min_cluster_size=ONLY_FEATURE_PARAMS[ego]['OPTICS']['min_cluster_size'])\n",
    "    optics.fit(data)\n",
    "    optics_labels = get_labels_dir(optics.labels_, index_dict)\n",
    "\n",
    "    return kmeans_labels, dbscan_labels, meanshift_labels, gmm_labels, hier_labels, optics_labels\n",
    "\n",
    "# Only edges models\n",
    "def only_edges_models(G):\n",
    "    # Greedy modularity communities\n",
    "\n",
    "    results_greedy = nxcom.greedy_modularity_communities(G)\n",
    "\n",
    "    # newman modularity communities\n",
    "    results_newman = nxcom.girvan_newman(G)\n",
    "\n",
    "    # Louvain modularity communities\n",
    "    results_partition = community_louvain.best_partition(G)\n",
    "\n",
    "     # Label propagation communities\n",
    "    results_label_propagation = list(nxcom.label_propagation_communities(G))\n",
    "\n",
    "    # Asyn_lpa_communities communities\n",
    "    results_asyn_lpa_communities = list(nxcom.asyn_lpa_communities(G))\n",
    "\n",
    "\n",
    "    return results_greedy, results_newman, results_partition, results_label_propagation, results_asyn_lpa_communities\n",
    "\n",
    "# Edges and features models\n",
    "def edges_and_features_models(data, labels):\n",
    "    pass\n",
    "\n",
    "# Getting dir of the labels from model results\n",
    "def get_labels_dir(labels, index_dict):\n",
    "    labels_dir = {}\n",
    "    for i in range(len(labels)):\n",
    "        labels_dir[index_dict[i]] = labels[i]\n",
    "    return labels_dir\n",
    "\n",
    "# Visualize the results\n",
    "def visualize(G, labels, model_name):\n",
    "    # Assign labels as node attributes for visualization\n",
    "    nx.set_node_attributes(G, labels, 'label')\n",
    "\n",
    "    # Automatically assign colors based on unique labels\n",
    "    unique_labels = set(labels.values())\n",
    "    \n",
    "    # Create a colormap that has as many colors as there are unique labels\n",
    "    color_map = plt.get_cmap('viridis', len(unique_labels))\n",
    "    # Assign a color to each label\n",
    "    label_to_color = {label: color_map(i) for i, label in enumerate(unique_labels)}\n",
    "    # Apply the colors to each node based on its label\n",
    "    node_colors = [label_to_color[G.nodes[node]['label']] for node in G.nodes]\n",
    "\n",
    "    # Visualize the graph\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    nx.draw(G, with_labels=True, node_color=node_colors, node_size=800, font_weight='bold', cmap=color_map)\n",
    "    \n",
    "    # Add the model name to the plot title\n",
    "    plt.title(f'Network Visualization - {model_name}')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Extract the data from the text\n",
    "def extract_data(text):\n",
    "    data = []\n",
    "    for line in text:\n",
    "        line = line.split()\n",
    "        data.append([float(line[0]), float(line[1])])\n",
    "    return np.array(data)\n",
    "    \n",
    "# Test labels\n",
    "def test_labels_results(labels, test_labels):\n",
    "    list_labels = []\n",
    "    list_test_labels = []\n",
    "    for i in test_labels.keys():\n",
    "        list_labels.append(labels[i])\n",
    "        list_test_labels.append(test_labels[i])\n",
    "    \n",
    "    labels = np.array(list_labels)\n",
    "    test_labels = np.array(list_test_labels)\n",
    "    ars = metrics.adjusted_rand_score(test_labels, labels)\n",
    "    nmis = metrics.normalized_mutual_info_score(test_labels, labels)\n",
    "    hs = metrics.homogeneity_score(test_labels, labels)\n",
    "    cs = metrics.completeness_score(test_labels, labels)\n",
    "    vs = metrics.v_measure_score(test_labels, labels)\n",
    "    ber = optimal_BER_matching(labels, test_labels)\n",
    "    # print(\"Adjusted Rand Score: \", ars)\n",
    "    # print(\"Normalized Mutual Info Score: \", nmis)\n",
    "    # print(\"Homogeneity Score: \", hs)\n",
    "    # print(\"Completeness Score: \", cs)\n",
    "    # print(\"V Measure Score: \", vs)\n",
    "\n",
    "    return ars, nmis, hs, cs, vs, ber\n",
    "\n",
    "# Get the data from greedy modularity communities\n",
    "def get_data_from_communities(results_greedy):\n",
    "    labels = {}\n",
    "    for i in range(len(results_greedy)):\n",
    "        for j in results_greedy[i]:\n",
    "            labels[j] = i\n",
    "    return labels\n",
    "\n",
    "    \n",
    "# Read labels\n",
    "def read_labels(filename):\n",
    "    labels = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            words = line.split()\n",
    "            for word in words[1:]:  # Skip the circle name\n",
    "                labels[word] = i  # Assign the label\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# Get nessasary labels\n",
    "def get_nessasary_labels(test_labels, labels):\n",
    "    sorted_keys = sorted(labels.keys())\n",
    "\n",
    "    final_test_labels = [test_labels[i - 1] for i in  sorted_keys]\n",
    "    final_labels = [labels[i] for i in sorted_keys]\n",
    "    return final_test_labels, final_labels\n",
    "\n",
    "# Create data \n",
    "def create_data(filename):\n",
    "    data = np.loadtxt(filename, delimiter=\" \")\n",
    "    final_data = []\n",
    "    final_data_dict = {}\n",
    "    index_dict = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        final_data.append(data[i][1:])\n",
    "        final_data_dict[str(int(data[i][0]))] = data[i][1:]\n",
    "        index_dict.append(str(int(data[i][0])))\n",
    "    return final_data, final_data_dict, index_dict\n",
    "\n",
    "# Calculate BER\n",
    "def optimal_BER_matching(labels, test_labels):\n",
    "    \"\"\"\n",
    "    Find the optimal matching between predicted and ground-truth circles to minimize BER.\n",
    "\n",
    "    Parameters:\n",
    "    - predicted_circles: list of sets, predicted circles.\n",
    "    - ground_truth_circles: list of sets, ground-truth circles.\n",
    "    - U: set, universal set of all nodes in the network.\n",
    "\n",
    "    Returns:\n",
    "    - Total optimal BER score after matching.\n",
    "    \"\"\"\n",
    "    cost_matrix = []\n",
    "    predicted_circles = dict_to_circles(labels)\n",
    "    ground_truth_circles = dict_to_circles(test_labels)\n",
    "    U = set(range(len(labels)))\n",
    "\n",
    "    for C in predicted_circles:\n",
    "        row = [calculate_BER(C, C_bar, U) for C_bar in ground_truth_circles]\n",
    "        cost_matrix.append(row)\n",
    "        \n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    optimal_BER = sum(1 - cost_matrix[row][col] for row, col in zip(row_ind, col_ind)) / len(predicted_circles)\n",
    "    return optimal_BER\n",
    "\n",
    "\n",
    "# Calculate BER\n",
    "def calculate_BER(C, C_bar, U):\n",
    "    \"\"\"\n",
    "    Calculate the Balanced Error Rate (BER) between two circles.\n",
    "\n",
    "    Parameters:\n",
    "    - C: set, predicted circle.\n",
    "    - C_bar: set, ground-truth circle.\n",
    "    - U: set, universal set of all nodes in the network.\n",
    "\n",
    "    Returns:\n",
    "    - BER score.\n",
    "    \"\"\"\n",
    "    C_complement = U - C\n",
    "    C_bar_complement = U - C_bar\n",
    "\n",
    "    false_positives = len(C - C_bar) / len(C) if C else 0\n",
    "    false_negatives = len(C_bar_complement - C_complement) / len(C_bar_complement) if C_bar_complement else 0\n",
    "\n",
    "    BER = 0.5 * (false_positives + false_negatives)\n",
    "\n",
    "    return BER\n",
    "\n",
    "def dict_to_circles(label_dict):\n",
    "    \"\"\"\n",
    "    Convert a dictionary mapping nodes to labels into a list of sets, where each set\n",
    "    contains the nodes belonging to a specific label (circle).\n",
    "\n",
    "    Parameters:\n",
    "    - label_dict: Dictionary where keys are node identifiers and values are their labels.\n",
    "\n",
    "    Returns:\n",
    "    - List of sets, with each set representing a circle.\n",
    "    \"\"\"\n",
    "    circles = {}\n",
    "    for i in range(len(label_dict)):\n",
    "        label = label_dict[i]\n",
    "        if label not in circles:\n",
    "            circles[label] = set()\n",
    "        circles[label].add(i)\n",
    "    return list(circles.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our approach is to analyze the ego networks and use unsupervised learning methods to predict circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ego node: 1684\n",
      "Newman\n",
      "Size:  11\n"
     ]
    }
   ],
   "source": [
    "for ego in EGO_NODES:\n",
    "    print(\"Ego node:\", ego)\n",
    "\n",
    "    # Get the data\n",
    "    data, data_dict, index_dict = create_data(\"facebook/\" + str(ego) + \".feat\")\n",
    "    test_labels = read_labels(\"facebook/\" + str(ego) + \".circles\")\n",
    "    network_G = nx.read_edgelist(\"facebook/\" + str(ego) + \".edges\")\n",
    "\n",
    "    # There can be a case when we have a node without any connections\n",
    "    for i in index_dict:\n",
    "        if i not in network_G.nodes:\n",
    "            network_G.add_node(i)\n",
    "\n",
    "    # results = get_results(network_G, data, data_dict, index_dict, test_labels, ModelTypes.ONLY_FEEATURES, ego=ego, plot=True)\n",
    "    # results.to_csv(\"results/ONLY_FEATURES.\" + str(ego) + \".csv\")\n",
    "\n",
    "    results = get_results(network_G, data, data_dict, index_dict, test_labels, ModelTypes.ONLY_EDGES, ego=ego, plot=True)\n",
    "    \n",
    "    # Save the results to csv\n",
    "    results.to_csv(\"results/ONLY_EDGES.\" + str(ego) + \".csv\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets create a model that will work with graph architecture and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
