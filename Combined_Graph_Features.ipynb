{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from sklearn.cluster import KMeans, DBSCAN, MeanShift, AgglomerativeClustering, OPTICS,  estimate_bandwidth\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.cm as cm\n",
    "import networkx as nx\n",
    "import networkx.algorithms.community as nxcom\n",
    "import community as community_louvain\n",
    "from networkx.algorithms.community import k_clique_communities, modularity\n",
    "# Constants\n",
    "EGO_NODES = [0, 348, 414, 686, 698, 1684, 1912, 3437, 3980]\n",
    "\n",
    "ONLY_FEATURE_PARAMS = {0 : {'KMeans':{\"k\": 100}, 'DBSCAN':{\"eps\": 0.05, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 10, \"covariance_type\": 'full'}, 'Hier':{\"k\": 50, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 5, \"xi\": 0.05, \"min_cluster_size\": 0.05}},\n",
    "                       107 : {'KMeans':{\"k\": 100}, 'DBSCAN':{\"eps\": 0.07, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 3}, 'Hier':{\"k\": 50, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 6, \"xi\": 0.05, \"min_cluster_size\": 0.05}},\n",
    "                          348 : {'KMeans':{\"k\": 100}, 'DBSCAN':{\"eps\": 0.05, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 3}, 'Hier':{\"k\": 60, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 5, \"xi\": 0.05, \"min_cluster_size\": 0.05}},\n",
    "                            414 : {'KMeans':{\"k\": 100}, 'DBSCAN':{\"eps\": 0.05, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 3}, 'Hier':{\"k\": 70, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 5, \"xi\": 0.05, \"min_cluster_size\": 0.05}},\n",
    "                                686 : {'KMeans':{\"k\": 100}, 'DBSCAN':{\"eps\": 0.05, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 3}, 'Hier':{\"k\": 65, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 5, \"xi\": 0.05, \"min_cluster_size\": 0.05}},\n",
    "                                    698 : {'KMeans':{\"k\": 60}, 'DBSCAN':{\"eps\": 0.05, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 3}, 'Hier':{\"k\": 65, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 5, \"xi\": 0.05, \"min_cluster_size\": 0.05}},\n",
    "                                    1684 : {'KMeans':{\"k\": 100}, 'DBSCAN':{\"eps\": 0.05, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 3}, 'Hier':{\"k\": 80, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 5, \"xi\": 0.05, \"min_cluster_size\": 0.05}},\n",
    "                                        1912 : {'KMeans':{\"k\": 100}, 'DBSCAN':{\"eps\": 0.05, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 3}, 'Hier':{\"k\": 80, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 5, \"xi\": 0.05, \"min_cluster_size\": 0.05}},\n",
    "                                        3437 : {'KMeans':{\"k\": 100}, 'DBSCAN':{\"eps\": 0.05, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 3}, 'Hier':{\"k\": 80, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 5, \"xi\": 0.05, \"min_cluster_size\": 0.05}},\n",
    "                                            3980 : {'KMeans':{\"k\": 58}, 'DBSCAN':{\"eps\": 0.05, \"min_samples\": 1}, 'MeanShift':{}, 'GMM':{\"k\": 3}, 'Hier':{\"k\": 58, \"affinity\": \"euclidean\", \"linkage\": \"ward\"}, 'OPTICS':{\"min_samples\": 5, \"xi\": 0.05, \"min_cluster_size\": 0.05}}}\n",
    "\n",
    "# Model types\n",
    "class Models(Enum):\n",
    "    KMEANS = 1\n",
    "    DBSCAN = 2\n",
    "    MEANSHIFT = 3\n",
    "    GMM = 4\n",
    "    HIERS = 5\n",
    "    Greedy = 6\n",
    "\n",
    "\n",
    "class ModelTypes(Enum):\n",
    "    ONLY_FEEATURES = 1\n",
    "    ONLY_EDGES = 2\n",
    "    EDGES_AND_FEATURES = 3\n",
    "\n",
    "# Unsupervised models\n",
    "class UnsupservisedModels:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_model(self, model):\n",
    "        \n",
    "        if model == Models.KMEANS:\n",
    "            return My_KMeans()\n",
    "        elif model == Models.DBSCAN:\n",
    "            return MY_DBSCAN()\n",
    "        # elif model == Models.MEANSHIFT:\n",
    "        #     return MeanShift()\n",
    "        # elif model == Models.GMM:\n",
    "        #     return GaussianMixture()\n",
    "        # elif model == Models.HIERS:\n",
    "        #     return AgglomerativeClustering()\n",
    "        else:\n",
    "            raise ValueError(\"Model not found\")\n",
    "\n",
    "# KMeans\n",
    "class My_KMeans:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        print(\"KMeans initialized\")\n",
    "        \n",
    "    def train(self, data):\n",
    "        print(\"Training KMeans\")\n",
    "        self.model = KMeans(n_clusters=self.k)\n",
    "\n",
    "        self.data = data\n",
    "        self.model.fit(data)\n",
    "\n",
    "        self.centroids = self.model.cluster_centers_\n",
    "        self.labels = self.model.labels_\n",
    "\n",
    "    # Get results\n",
    "    def get_results(self):\n",
    "        return self.labels\n",
    "    \n",
    "# DBSCAN\n",
    "class My_DBSCAN:\n",
    "    def __init__(self, eps, min_samples):\n",
    "        self.eps = 0.5\n",
    "        self.min_samples = 3\n",
    "        # Initialize DBSCAN\n",
    "        self.model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "\n",
    "    def train(self, data):\n",
    "        self.data = data\n",
    "        self.model.fit(data)\n",
    "\n",
    "        self.labels = self.model.labels_\n",
    "\n",
    "    # Get results\n",
    "    def get_results(self):\n",
    "        return self.labels\n",
    "    \n",
    "# MeanShift\n",
    "class My_MeanShift:\n",
    "    def __init__(self) -> None:\n",
    "        self.model = MeanShift()\n",
    "\n",
    "    def train(self, data):\n",
    "        self.data = data\n",
    "        self.model.fit(data)\n",
    "\n",
    "        self.labels = self.model.labels_\n",
    "\n",
    "    # Get results\n",
    "    def get_results(self):\n",
    "        return self.labels\n",
    "    \n",
    "# GaussianMixture\n",
    "class My_GMM:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.model = GaussianMixture(self.k)\n",
    "\n",
    "    def train(self, data):\n",
    "        self.data = data\n",
    "        self.model.fit(data)\n",
    "\n",
    "        self.labels = self.model.predict(data)\n",
    "\n",
    "    # Get results\n",
    "    def get_results(self):\n",
    "        return self.labels\n",
    "    \n",
    "# AgglomerativeClustering\n",
    "class My_Hier:\n",
    "    def __init__(self, k, affinity, linkage):\n",
    "        self.k = k\n",
    "        self.affinity = affinity\n",
    "        self.linkage = linkage\n",
    "\n",
    "        self.model = AgglomerativeClustering(n_clusters=self.k, affinity=self.affinity, linkage=self.linkage)\n",
    "\n",
    "    def train(self, data):\n",
    "        self.data = data\n",
    "        self.model.fit(data)\n",
    "\n",
    "        self.labels = self.model.labels_\n",
    "\n",
    "    # Get results\n",
    "    def get_results(self):\n",
    "        return self.labels\n",
    "    \n",
    "# OPTICS\n",
    "class My_OPTICS:\n",
    "    def __init__(self, min_samples=5, xi=0.05, min_cluster_size=0.05):\n",
    "        self.model = OPTICS(min_samples=min_samples, xi=xi, min_cluster_size=min_cluster_size)\n",
    "\n",
    "    def train(self, data):\n",
    "        self.data = data\n",
    "        self.model.fit(data)\n",
    "        self.labels = self.model.labels_\n",
    "\n",
    "    def get_results(self):\n",
    "        return self.labels\n",
    "\n",
    "\n",
    "# Helpers\n",
    "def get_results(G, data, data_dict, index_dict, labels, model_type, ego, plot=False):\n",
    "    if model_type == ModelTypes.ONLY_FEEATURES:\n",
    "        kmeans_labels, dbscan_labels, meanshift_labels, gmm_labels, hier_labels, optics_labels = only_features_models(data, index_dict, ego)\n",
    "        if plot:\n",
    "            print('KMeans')\n",
    "            visualize(G, kmeans_labels, \"KMeans\")\n",
    "            print('DBSCAN')\n",
    "            visualize(G, dbscan_labels, \"DBSCAN\")\n",
    "            print('MeanShift')\n",
    "            visualize(G, meanshift_labels, \"MeanShift\")\n",
    "            print('GMM')\n",
    "            visualize(G, gmm_labels, \"GMM\")\n",
    "            print('Hier')\n",
    "            visualize(G, hier_labels, \"Hier\")\n",
    "            print('OPTICS')\n",
    "            visualize(G, optics_labels, \"OPTICS\")\n",
    "    \n",
    "        \n",
    "        ars_k_means, nmis_k_means, hs_k_means, cs_k_means, vs_k_means, ber_kmeans = test_labels_results(kmeans_labels, labels)\n",
    "        ars_dbscan, nmis_dbscan, hs_dbscan, cs_dbscan, vs_dbscan, ber_dbscan = test_labels_results(dbscan_labels, labels)\n",
    "        ars_meanshift, nmis_meanshift, hs_meanshift, cs_meanshift, vs_meanshift, ber_meanshift = test_labels_results(meanshift_labels, labels)\n",
    "        ars_gmm, nmis_gmm, hs_gmm, cs_gmm, vs_gmm, ber_gmm = test_labels_results(gmm_labels, labels)\n",
    "        ars_hier, nmis_hier, hs_hier, cs_hier, vs_hier, ber_hier = test_labels_results(hier_labels, labels)\n",
    "        ars_optics, nmis_optics, hs_optics, cs_optics, vs_optics, ber_optics = test_labels_results(optics_labels, labels)\n",
    "        \n",
    "        # Create dataframe for the results with index as the model name and columns as the metrics\n",
    "        results =  pd.DataFrame(index=['KMeans', 'DBSCAN', 'MeanShift', 'GMM', 'Hier', 'OPTICS'],\n",
    "                                 columns=['Adjusted Rand Score', 'Normalized Mutual Info Score', 'Homogeneity Score',\n",
    "                                            'Completeness Score', 'V Measure Score', 'BER'])\n",
    "        results.loc['KMeans'] = [ars_k_means, nmis_k_means, hs_k_means, cs_k_means, vs_k_means, ber_kmeans]\n",
    "        results.loc['DBSCAN'] = [ars_dbscan, nmis_dbscan, hs_dbscan, cs_dbscan, vs_dbscan, ber_dbscan]\n",
    "        results.loc['MeanShift'] = [ars_meanshift, nmis_meanshift, hs_meanshift, cs_meanshift, vs_meanshift, ber_meanshift]\n",
    "        results.loc['GMM'] = [ars_gmm, nmis_gmm, hs_gmm, cs_gmm, vs_gmm, ber_gmm]\n",
    "        results.loc['Hier'] = [ars_hier, nmis_hier, hs_hier, cs_hier, vs_hier, ber_hier]\n",
    "        results.loc['OPTICS'] = [ars_optics, nmis_optics, hs_optics, cs_optics, vs_optics, ber_optics]\n",
    "    \n",
    "\n",
    "        return results\n",
    "        \n",
    "        # return pass\n",
    "    \n",
    "    elif model_type == ModelTypes.ONLY_EDGES:\n",
    "\n",
    "        results_greedy, results_newman, results_partition, results_label_propagation, results_asyn_lpa_communities = only_edges_models(G)\n",
    "\n",
    "\n",
    "        resss= get_data_from_communities(results_greedy)\n",
    "        ars_greedy, nmis_greedy, hs_greedy, cs_greedy, vs_greedy, ber_greedy = test_labels_results(resss, labels)\n",
    "        \n",
    "\n",
    "        max_modularity = None\n",
    "        best_partition = None\n",
    "        for communities in results_newman:\n",
    "            partition = list(map(set, communities))\n",
    "            modularity_score = modularity(G, partition)\n",
    "            if max_modularity is None or modularity_score > max_modularity:\n",
    "                max_modularity = modularity_score\n",
    "                best_partition = partition\n",
    "        \n",
    "        ars_newman, nmis_newman, hs_newman, cs_newman, vs_newman, ber_newman = test_labels_results(get_data_from_communities(best_partition), labels)\n",
    "\n",
    "        ars_partition, nmis_partition, hs_partition, cs_partition, vs_partition, ber_partition = test_labels_results(results_partition, labels)\n",
    "\n",
    "        ars_propogation, nmis_propogation, hs_propogation, cs_propogation, vs_propogation, ber_propogation = test_labels_results(get_data_from_communities(results_label_propagation), labels)\n",
    "\n",
    "        ars_lpa, nmis_lpa, hs_lpa, cs_lpa, vs_lpa, ber_lpa = test_labels_results(get_data_from_communities(results_asyn_lpa_communities), labels)\n",
    "       \n",
    "        if plot:\n",
    "            print('Greedy')\n",
    "            visualize(G, resss, \"Greedy\")\n",
    "            print('Newman')\n",
    "            visualize(G, get_data_from_communities(best_partition), \"Newman\")\n",
    "            print('Partition')\n",
    "            visualize(G, results_partition, \"Partition\")\n",
    "            print('Label Propogation')\n",
    "            visualize(G, get_data_from_communities(results_label_propagation), \"Label Propogation\")\n",
    "            print('LPA')\n",
    "            visualize(G, get_data_from_communities(results_asyn_lpa_communities), \"LPA\")\n",
    "        \n",
    "        # Create dataframe for the results with index as the model name and columns as the metrics\n",
    "        results =  pd.DataFrame(index=['Greedy', 'Newman', 'Partition', 'Label Propogation', 'LPA'],\n",
    "                                 columns=['Adjusted Rand Score', 'Normalized Mutual Info Score', 'Homogeneity Score',\n",
    "                                            'Completeness Score', 'V Measure Score', 'BER'])\n",
    "        \n",
    "        results.loc['Greedy'] = [ars_greedy, nmis_greedy, hs_greedy, cs_greedy, vs_greedy, ber_greedy]\n",
    "        results.loc['Newman'] = [ars_newman, nmis_newman, hs_newman, cs_newman, vs_newman, ber_newman]\n",
    "        results.loc['Partition'] = [ars_partition, nmis_partition, hs_partition, cs_partition, vs_partition, ber_partition]\n",
    "        results.loc['Label Propogation'] = [ars_propogation, nmis_propogation, hs_propogation, cs_propogation, vs_propogation, ber_propogation]\n",
    "        results.loc['LPA'] = [ars_lpa, nmis_lpa, hs_lpa, cs_lpa, vs_lpa, ber_lpa]\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "        \n",
    "        # return resss\n",
    "        # return only_edges_models(data, labels)\n",
    "    elif model_type == ModelTypes.EDGES_AND_FEATURES:\n",
    "        return edges_and_features_models(data, labels)\n",
    "    else:\n",
    "        raise ValueError(\"Model type not found\")\n",
    "    \n",
    "# Only features models\n",
    "def only_features_models(data, index_dict, ego):\n",
    "    \"\"\"\n",
    "    data - data\n",
    "    labels - labels\n",
    "    \n",
    "    We are going to use KMeans, DBSCAN, MeanShift, GaussianMixture, AgglomerativeClustering\n",
    "    \"\"\"\n",
    "\n",
    "    # KMeans\n",
    "    kmeans = KMeans(n_clusters=ONLY_FEATURE_PARAMS[ego]['KMeans']['k'])\n",
    "    kmeans.fit(data)\n",
    "    kmeans_labels = get_labels_dir(kmeans.labels_, index_dict)\n",
    "\n",
    "    # DBSCAN\n",
    "    dbscan = DBSCAN(eps=ONLY_FEATURE_PARAMS[ego]['DBSCAN']['eps'], min_samples=ONLY_FEATURE_PARAMS[ego]['DBSCAN']['min_samples'])\n",
    "    dbscan.fit(data)\n",
    "    dbscan_labels = get_labels_dir(dbscan.labels_, index_dict)\n",
    "\n",
    "    # MeanShift\n",
    "    bandwidth = estimate_bandwidth(data)\n",
    "\n",
    "    meanshift = MeanShift(bandwidth=bandwidth)\n",
    "    meanshift.fit(data)\n",
    "    meanshift_labels = get_labels_dir(meanshift.labels_, index_dict)\n",
    "\n",
    "    # GaussianMixture\n",
    "    gmm = GaussianMixture(n_components=3, covariance_type='tied', tol=0.001, reg_covar=1e-06, init_params='random', max_iter=200, n_init=3)\n",
    "    gmm.fit(data)\n",
    "    gmm_labels = get_labels_dir(gmm.predict(data), index_dict)\n",
    "\n",
    "    # AgglomerativeClustering\n",
    "    hier = AgglomerativeClustering(n_clusters=ONLY_FEATURE_PARAMS[ego]['Hier']['k'], affinity=ONLY_FEATURE_PARAMS[ego]['Hier']['affinity'], linkage=ONLY_FEATURE_PARAMS[ego]['Hier']['linkage'])\n",
    "    hier.fit(data)\n",
    "    hier_labels = get_labels_dir(hier.labels_, index_dict)\n",
    "\n",
    "    # OPTICS\n",
    "    optics = OPTICS(min_samples=ONLY_FEATURE_PARAMS[ego]['OPTICS']['min_samples'], xi=ONLY_FEATURE_PARAMS[ego]['OPTICS']['xi'], min_cluster_size=ONLY_FEATURE_PARAMS[ego]['OPTICS']['min_cluster_size'])\n",
    "    optics.fit(data)\n",
    "    optics_labels = get_labels_dir(optics.labels_, index_dict)\n",
    "\n",
    "    return kmeans_labels, dbscan_labels, meanshift_labels, gmm_labels, hier_labels, optics_labels\n",
    "\n",
    "# Only edges models\n",
    "def only_edges_models(G):\n",
    "    # Greedy modularity communities\n",
    "\n",
    "    results_greedy = nxcom.greedy_modularity_communities(G)\n",
    "\n",
    "    # newman modularity communities\n",
    "    results_newman = nxcom.girvan_newman(G)\n",
    "\n",
    "    # Louvain modularity communities\n",
    "    results_partition = community_louvain.best_partition(G)\n",
    "\n",
    "     # Label propagation communities\n",
    "    results_label_propagation = list(nxcom.label_propagation_communities(G))\n",
    "\n",
    "    # Asyn_lpa_communities communities\n",
    "    results_asyn_lpa_communities = list(nxcom.asyn_lpa_communities(G))\n",
    "\n",
    "\n",
    "    return results_greedy, results_newman, results_partition, results_label_propagation, results_asyn_lpa_communities\n",
    "\n",
    "# Edges and features models\n",
    "def edges_and_features_models(data, labels):\n",
    "    pass\n",
    "\n",
    "# Getting dir of the labels from model results\n",
    "def get_labels_dir(labels, index_dict):\n",
    "    labels_dir = {}\n",
    "    for i in range(len(labels)):\n",
    "        labels_dir[index_dict[i]] = labels[i]\n",
    "    return labels_dir\n",
    "\n",
    "# Visualize the results\n",
    "def visualize(G, labels, model_name):\n",
    "    # Assign labels as node attributes for visualization\n",
    "    nx.set_node_attributes(G, labels, 'label')\n",
    "\n",
    "    # Automatically assign colors based on unique labels\n",
    "    unique_labels = set(labels.values())\n",
    "    \n",
    "    # Create a colormap that has as many colors as there are unique labels\n",
    "    color_map = plt.get_cmap('viridis', len(unique_labels))\n",
    "    # Assign a color to each label\n",
    "    label_to_color = {label: color_map(i) for i, label in enumerate(unique_labels)}\n",
    "    # Apply the colors to each node based on its label\n",
    "    node_colors = [label_to_color[G.nodes[node]['label']] for node in G.nodes]\n",
    "\n",
    "    # Visualize the graph\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    nx.draw(G, with_labels=True, node_color=node_colors, node_size=800, font_weight='bold', cmap=color_map)\n",
    "    \n",
    "    # Add the model name to the plot title\n",
    "    plt.title(f'Network Visualization - {model_name}')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Extract the data from the text\n",
    "def extract_data(text):\n",
    "    data = []\n",
    "    for line in text:\n",
    "        line = line.split()\n",
    "        data.append([float(line[0]), float(line[1])])\n",
    "    return np.array(data)\n",
    "    \n",
    "# Test labels\n",
    "def test_labels_results(labels, test_labels):\n",
    "    list_labels = []\n",
    "    list_test_labels = []\n",
    "    for i in test_labels.keys():\n",
    "        list_labels.append(labels[i])\n",
    "        list_test_labels.append(test_labels[i])\n",
    "    \n",
    "    labels = np.array(list_labels)\n",
    "    test_labels = np.array(list_test_labels)\n",
    "    ars = metrics.adjusted_rand_score(test_labels, labels)\n",
    "    nmis = metrics.normalized_mutual_info_score(test_labels, labels)\n",
    "    hs = metrics.homogeneity_score(test_labels, labels)\n",
    "    cs = metrics.completeness_score(test_labels, labels)\n",
    "    vs = metrics.v_measure_score(test_labels, labels)\n",
    "    ber = optimal_BER_matching(labels, test_labels)\n",
    "    # print(\"Adjusted Rand Score: \", ars)\n",
    "    # print(\"Normalized Mutual Info Score: \", nmis)\n",
    "    # print(\"Homogeneity Score: \", hs)\n",
    "    # print(\"Completeness Score: \", cs)\n",
    "    # print(\"V Measure Score: \", vs)\n",
    "\n",
    "    return ars, nmis, hs, cs, vs, ber\n",
    "\n",
    "# Get the data from greedy modularity communities\n",
    "def get_data_from_communities(results_greedy):\n",
    "    labels = {}\n",
    "    for i in range(len(results_greedy)):\n",
    "        for j in results_greedy[i]:\n",
    "            labels[j] = i\n",
    "    return labels\n",
    "\n",
    "    \n",
    "# Read labels\n",
    "def read_labels(filename):\n",
    "    labels = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            words = line.split()\n",
    "            for word in words[1:]:  # Skip the circle name\n",
    "                labels[word] = i  # Assign the label\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# Get nessasary labels\n",
    "def get_nessasary_labels(test_labels, labels):\n",
    "    sorted_keys = sorted(labels.keys())\n",
    "\n",
    "    final_test_labels = [test_labels[i - 1] for i in  sorted_keys]\n",
    "    final_labels = [labels[i] for i in sorted_keys]\n",
    "    return final_test_labels, final_labels\n",
    "\n",
    "# Create data \n",
    "def create_data(filename):\n",
    "    data = np.loadtxt(filename, delimiter=\" \")\n",
    "    final_data = []\n",
    "    final_data_dict = {}\n",
    "    index_dict = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        final_data.append(data[i][1:])\n",
    "        final_data_dict[str(int(data[i][0]))] = data[i][1:]\n",
    "        index_dict.append(str(int(data[i][0])))\n",
    "    return final_data, final_data_dict, index_dict\n",
    "\n",
    "# Calculate BER\n",
    "def optimal_BER_matching(labels, test_labels):\n",
    "    \"\"\"\n",
    "    Find the optimal matching between predicted and ground-truth circles to minimize BER.\n",
    "\n",
    "    Parameters:\n",
    "    - predicted_circles: list of sets, predicted circles.\n",
    "    - ground_truth_circles: list of sets, ground-truth circles.\n",
    "    - U: set, universal set of all nodes in the network.\n",
    "\n",
    "    Returns:\n",
    "    - Total optimal BER score after matching.\n",
    "    \"\"\"\n",
    "    cost_matrix = []\n",
    "    predicted_circles = dict_to_circles(labels)\n",
    "    ground_truth_circles = dict_to_circles(test_labels)\n",
    "    U = set(range(len(labels)))\n",
    "\n",
    "    for C in predicted_circles:\n",
    "        row = [calculate_BER(C, C_bar, U) for C_bar in ground_truth_circles]\n",
    "        cost_matrix.append(row)\n",
    "        \n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    optimal_BER = sum(1 - cost_matrix[row][col] for row, col in zip(row_ind, col_ind)) / len(predicted_circles)\n",
    "    return optimal_BER\n",
    "\n",
    "\n",
    "# Calculate BER\n",
    "def calculate_BER(C, C_bar, U):\n",
    "    \"\"\"\n",
    "    Calculate the Balanced Error Rate (BER) between two circles.\n",
    "\n",
    "    Parameters:\n",
    "    - C: set, predicted circle.\n",
    "    - C_bar: set, ground-truth circle.\n",
    "    - U: set, universal set of all nodes in the network.\n",
    "\n",
    "    Returns:\n",
    "    - BER score.\n",
    "    \"\"\"\n",
    "    C_complement = U - C\n",
    "    C_bar_complement = U - C_bar\n",
    "\n",
    "    false_positives = len(C - C_bar) / len(C) if C else 0\n",
    "    false_negatives = len(C_bar_complement - C_complement) / len(C_bar_complement) if C_bar_complement else 0\n",
    "\n",
    "    BER = 0.5 * (false_positives + false_negatives)\n",
    "\n",
    "    return BER\n",
    "\n",
    "def dict_to_circles(label_dict):\n",
    "    \"\"\"\n",
    "    Convert a dictionary mapping nodes to labels into a list of sets, where each set\n",
    "    contains the nodes belonging to a specific label (circle).\n",
    "\n",
    "    Parameters:\n",
    "    - label_dict: Dictionary where keys are node identifiers and values are their labels.\n",
    "\n",
    "    Returns:\n",
    "    - List of sets, with each set representing a circle.\n",
    "    \"\"\"\n",
    "    circles = {}\n",
    "    for i in range(len(label_dict)):\n",
    "        label = label_dict[i]\n",
    "        if label not in circles:\n",
    "            circles[label] = set()\n",
    "        circles[label].add(i)\n",
    "    return list(circles.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego = 0\n",
    "data, data_dict, index_dict = create_data(\"facebook/\" + str(ego) + \".feat\")\n",
    "test_labels = read_labels(\"facebook/\" + str(ego) + \".circles\")\n",
    "network_G = nx.read_edgelist(\"facebook/\" + str(ego) + \".edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data_dict.keys():\n",
    "    if key in network_G.nodes():\n",
    "        network_G.nodes[key]['features'] = data_dict[key]\n",
    "    else:\n",
    "        network_G.add_node(key, features=data_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def girvan_newman_with_features(G, num_communities):\n",
    "    # Calculate the edge betweenness centrality of each edge using the weighted graph.\n",
    "    num_edges = G.number_of_edges()\n",
    "    while G.number_of_edges() > num_edges - num_communities:\n",
    "        # Calculate the similarity between the feature vectors of each pair of nodes connected by an edge.\n",
    "        for u, v in G.edges():\n",
    "            similarity = sum([a * b for a, b in zip(G.nodes[u]['features'], G.nodes[v]['features'])])\n",
    "            G.edges[u, v]['weight'] = similarity\n",
    "\n",
    "        # Calculate the edge betweenness centrality of each edge using the weighted graph.\n",
    "        betweenness = nx.edge_betweenness_centrality(G, weight='weight')\n",
    "        max_edge = max(betweenness, key=betweenness.get)\n",
    "        G.remove_edge(*max_edge)\n",
    "\n",
    "    # Extract the communities from the graph.\n",
    "    communities = list(nx.connected_components(G))\n",
    "\n",
    "    return communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'1',\n",
       "  '10',\n",
       "  '100',\n",
       "  '101',\n",
       "  '102',\n",
       "  '103',\n",
       "  '104',\n",
       "  '105',\n",
       "  '106',\n",
       "  '107',\n",
       "  '108',\n",
       "  '109',\n",
       "  '110',\n",
       "  '111',\n",
       "  '112',\n",
       "  '113',\n",
       "  '115',\n",
       "  '116',\n",
       "  '117',\n",
       "  '118',\n",
       "  '119',\n",
       "  '120',\n",
       "  '121',\n",
       "  '122',\n",
       "  '123',\n",
       "  '124',\n",
       "  '125',\n",
       "  '126',\n",
       "  '127',\n",
       "  '128',\n",
       "  '129',\n",
       "  '13',\n",
       "  '130',\n",
       "  '131',\n",
       "  '132',\n",
       "  '133',\n",
       "  '134',\n",
       "  '135',\n",
       "  '136',\n",
       "  '137',\n",
       "  '138',\n",
       "  '139',\n",
       "  '14',\n",
       "  '140',\n",
       "  '141',\n",
       "  '142',\n",
       "  '143',\n",
       "  '144',\n",
       "  '146',\n",
       "  '147',\n",
       "  '148',\n",
       "  '149',\n",
       "  '150',\n",
       "  '151',\n",
       "  '152',\n",
       "  '153',\n",
       "  '154',\n",
       "  '155',\n",
       "  '156',\n",
       "  '157',\n",
       "  '158',\n",
       "  '159',\n",
       "  '16',\n",
       "  '160',\n",
       "  '161',\n",
       "  '162',\n",
       "  '163',\n",
       "  '164',\n",
       "  '165',\n",
       "  '166',\n",
       "  '167',\n",
       "  '168',\n",
       "  '169',\n",
       "  '17',\n",
       "  '170',\n",
       "  '171',\n",
       "  '172',\n",
       "  '173',\n",
       "  '174',\n",
       "  '175',\n",
       "  '176',\n",
       "  '177',\n",
       "  '178',\n",
       "  '180',\n",
       "  '181',\n",
       "  '182',\n",
       "  '183',\n",
       "  '184',\n",
       "  '185',\n",
       "  '186',\n",
       "  '187',\n",
       "  '188',\n",
       "  '189',\n",
       "  '19',\n",
       "  '190',\n",
       "  '191',\n",
       "  '192',\n",
       "  '193',\n",
       "  '194',\n",
       "  '195',\n",
       "  '196',\n",
       "  '197',\n",
       "  '198',\n",
       "  '199',\n",
       "  '2',\n",
       "  '20',\n",
       "  '200',\n",
       "  '201',\n",
       "  '202',\n",
       "  '203',\n",
       "  '204',\n",
       "  '205',\n",
       "  '206',\n",
       "  '207',\n",
       "  '208',\n",
       "  '21',\n",
       "  '211',\n",
       "  '212',\n",
       "  '213',\n",
       "  '214',\n",
       "  '216',\n",
       "  '217',\n",
       "  '218',\n",
       "  '219',\n",
       "  '22',\n",
       "  '220',\n",
       "  '221',\n",
       "  '222',\n",
       "  '223',\n",
       "  '224',\n",
       "  '225',\n",
       "  '226',\n",
       "  '227',\n",
       "  '228',\n",
       "  '229',\n",
       "  '23',\n",
       "  '230',\n",
       "  '231',\n",
       "  '232',\n",
       "  '234',\n",
       "  '235',\n",
       "  '236',\n",
       "  '237',\n",
       "  '238',\n",
       "  '239',\n",
       "  '24',\n",
       "  '240',\n",
       "  '241',\n",
       "  '242',\n",
       "  '243',\n",
       "  '245',\n",
       "  '246',\n",
       "  '247',\n",
       "  '248',\n",
       "  '249',\n",
       "  '25',\n",
       "  '250',\n",
       "  '251',\n",
       "  '252',\n",
       "  '253',\n",
       "  '254',\n",
       "  '255',\n",
       "  '257',\n",
       "  '258',\n",
       "  '259',\n",
       "  '26',\n",
       "  '260',\n",
       "  '261',\n",
       "  '262',\n",
       "  '263',\n",
       "  '264',\n",
       "  '265',\n",
       "  '266',\n",
       "  '267',\n",
       "  '268',\n",
       "  '269',\n",
       "  '27',\n",
       "  '270',\n",
       "  '271',\n",
       "  '272',\n",
       "  '273',\n",
       "  '274',\n",
       "  '275',\n",
       "  '276',\n",
       "  '277',\n",
       "  '278',\n",
       "  '279',\n",
       "  '28',\n",
       "  '280',\n",
       "  '281',\n",
       "  '283',\n",
       "  '284',\n",
       "  '285',\n",
       "  '286',\n",
       "  '288',\n",
       "  '289',\n",
       "  '29',\n",
       "  '290',\n",
       "  '291',\n",
       "  '293',\n",
       "  '294',\n",
       "  '295',\n",
       "  '296',\n",
       "  '297',\n",
       "  '298',\n",
       "  '299',\n",
       "  '3',\n",
       "  '30',\n",
       "  '300',\n",
       "  '301',\n",
       "  '302',\n",
       "  '303',\n",
       "  '304',\n",
       "  '305',\n",
       "  '306',\n",
       "  '307',\n",
       "  '308',\n",
       "  '309',\n",
       "  '31',\n",
       "  '310',\n",
       "  '311',\n",
       "  '312',\n",
       "  '313',\n",
       "  '314',\n",
       "  '315',\n",
       "  '316',\n",
       "  '317',\n",
       "  '318',\n",
       "  '319',\n",
       "  '32',\n",
       "  '320',\n",
       "  '321',\n",
       "  '322',\n",
       "  '323',\n",
       "  '324',\n",
       "  '325',\n",
       "  '326',\n",
       "  '327',\n",
       "  '328',\n",
       "  '329',\n",
       "  '330',\n",
       "  '331',\n",
       "  '332',\n",
       "  '333',\n",
       "  '334',\n",
       "  '336',\n",
       "  '337',\n",
       "  '338',\n",
       "  '339',\n",
       "  '34',\n",
       "  '340',\n",
       "  '341',\n",
       "  '342',\n",
       "  '343',\n",
       "  '344',\n",
       "  '345',\n",
       "  '346',\n",
       "  '347',\n",
       "  '35',\n",
       "  '36',\n",
       "  '38',\n",
       "  '39',\n",
       "  '4',\n",
       "  '40',\n",
       "  '41',\n",
       "  '44',\n",
       "  '45',\n",
       "  '46',\n",
       "  '47',\n",
       "  '48',\n",
       "  '49',\n",
       "  '5',\n",
       "  '50',\n",
       "  '51',\n",
       "  '52',\n",
       "  '53',\n",
       "  '54',\n",
       "  '55',\n",
       "  '56',\n",
       "  '57',\n",
       "  '58',\n",
       "  '59',\n",
       "  '6',\n",
       "  '60',\n",
       "  '61',\n",
       "  '62',\n",
       "  '63',\n",
       "  '64',\n",
       "  '65',\n",
       "  '66',\n",
       "  '67',\n",
       "  '68',\n",
       "  '69',\n",
       "  '7',\n",
       "  '70',\n",
       "  '71',\n",
       "  '72',\n",
       "  '73',\n",
       "  '75',\n",
       "  '76',\n",
       "  '77',\n",
       "  '78',\n",
       "  '79',\n",
       "  '8',\n",
       "  '80',\n",
       "  '81',\n",
       "  '82',\n",
       "  '83',\n",
       "  '84',\n",
       "  '85',\n",
       "  '86',\n",
       "  '87',\n",
       "  '88',\n",
       "  '89',\n",
       "  '9',\n",
       "  '91',\n",
       "  '92',\n",
       "  '93',\n",
       "  '94',\n",
       "  '95',\n",
       "  '96',\n",
       "  '97',\n",
       "  '98',\n",
       "  '99'},\n",
       " {'33', '42'},\n",
       " {'145', '179', '90'},\n",
       " {'244', '282'},\n",
       " {'233', '256'},\n",
       " {'11'},\n",
       " {'12'},\n",
       " {'15'},\n",
       " {'18'},\n",
       " {'37'},\n",
       " {'43'},\n",
       " {'74'},\n",
       " {'114'},\n",
       " {'209'},\n",
       " {'210'},\n",
       " {'215'},\n",
       " {'287'},\n",
       " {'292'},\n",
       " {'335'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "girvan_newman_with_features(network_G, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.21179299896476722,\n",
       " 0.3619625896202325,\n",
       " 0.2971660422509327,\n",
       " 0.4628963464207499,\n",
       " 0.3619625896202324,\n",
       " 0.7231694875745962)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_results(get_data_from_communities(girvan_newman_with_features(network_G, 50)), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
